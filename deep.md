Loss function: $L(x, y) = \sum{abs(y_i - \haty_i)^2}
${\mathrm  {TSS}}=\sum _{{i=1}}^{{n}}\left(y_{{i}}-{\bar  {y}}\right)^{2}$
ReLU has a nice derivative (flat line)
$f(x)=x^{+}=\max(0,x)$
max(0, x)

softmax
$\sigma (\mathbf {z} )_{j}={\frac {e^{z_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}}$
function on vectors
$ \sigma(v) = \frac{e^{1/2}}{\sum{e^{1/2}}}$
multinomial logit

Gradient descent to minimize loss function
Get the derivative
Optimize the network in our feature space.
Calculate derivate of loss function
Chain rule from calculus

CNN versus fully connected

Acclimation function

Most common loss functions:
binomial log loss and mean square error
